{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91c1366-03fa-4d54-91ae-4065f4bd4abb",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ef8d4-63fe-4a39-8be4-62954ba533a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "os.environ[\"PATH\"] += os.pathsep +'C:\\Program Files\\Graphviz\\bin'\n",
    "#=============================================================================================\n",
    "#=============================================================================================\n",
    "# 读取数据\n",
    "data = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "# 去除无用数据，后3列是无用数据\n",
    "data = data[['v1', 'v2']]\n",
    "# 修改表头信息\n",
    "data = data.rename(columns={\"v1\":\"label\",\"v2\":\"text\"})\n",
    "# 去除标点符号及两个以上的空格\n",
    "data['text'] = data['text'].apply(lambda x:re.sub('[!@#$:).;,?&]', ' ', x.lower()))\n",
    "data['text'] = data['text'].apply(lambda x:re.sub(' ', ' ', x))\n",
    "# 单词转换为小写\n",
    "data['text'] = data['text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "# 去除停止词 ，如a、an、the、高频介词、连词、代词等\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# 分词处理，希望能够实现还原英文单词原型\n",
    "st = PorterStemmer()\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join([word for word in x.split()]))\n",
    "\n",
    "#分出训练集和测试集\n",
    "train=data[:5000]\n",
    "test=data[5000:]\n",
    "# 每个序列的最大长度，多了截断，少了补0\n",
    "max_sequence_length = 300\n",
    "\n",
    "#只保留频率最高的前20000个词\n",
    "num_words = 20000\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 100\n",
    "# 找出经常出现的单词，分词器\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(train.text)\n",
    "train_sequences = tokenizer.texts_to_sequences(train.text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.text)\n",
    "\n",
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# get only the top frequent words on train\n",
    "\n",
    "train_x = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "# get only the top frequent words on test\n",
    "test_x = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "\n",
    "# 标签向量化\n",
    "# [0,1]: ham;[1,0]:spam\n",
    "def lable_vectorize(labels):\n",
    "    label_vec = np.zeros([len(labels), 2])\n",
    "    for i, label in enumerate(labels):\n",
    "        if str(label) == 'ham':\n",
    "            label_vec[i][0] = 1\n",
    "        else:\n",
    "            label_vec[i][1] = 1\n",
    "    return label_vec\n",
    "\n",
    "\n",
    "train_y = lable_vectorize(train['label'])\n",
    "test_y = lable_vectorize(test['label'])\n",
    "X_train = np.reshape(train_x , (train_x .shape[0], train_x .shape[1], 1))\n",
    "X_test = np.reshape(test_x, (test_x .shape[0], test_x .shape[1], 1))\n",
    "print(\"加载数据完成\")\n",
    "#=============================================================================================\n",
    "#=============================================================================================\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 20\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_hidden,\n",
    "               batch_input_shape=(None, max_sequence_length, 1),\n",
    "               unroll=True))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "plot_model(model, to_file='lstm.png',show_shapes='True')\n",
    "\n",
    "adam = Adam(lr=learning_rate)\n",
    "model.summary()\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=training_iters,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, test_y))\n",
    "\n",
    "scores = model.evaluate(X_test, test_y, verbose=0)\n",
    "print('LSTM test score:', scores[0])\n",
    "print('LSTM test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717a46e-b122-417d-bab9-9b45540d1047",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806677ab-1ff3-4b3c-9c4e-f24ecc2e18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten, Dropout,BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from livelossplot import PlotLossesKeras\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep +'C:\\Program Files\\Graphviz\\bin'\n",
    "# 读取数据\n",
    "data = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "# 去除无用数据，后3列是无用数据\n",
    "data = data[['v1', 'v2']]\n",
    "# 修改表头信息\n",
    "data = data.rename(columns={\"v1\":\"label\",\"v2\":\"text\"})\n",
    "# 去除标点符号及两个以上的空格\n",
    "data['text'] = data['text'].apply(lambda x:re.sub('[!@#$:).;,?&]', ' ', x.lower()))\n",
    "data['text'] = data['text'].apply(lambda x:re.sub(' ', ' ', x))\n",
    "# 单词转换为小写\n",
    "data['text'] = data['text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "# 去除停止词 ，如a、an、the、高频介词、连词、代词等\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# 分词处理，希望能够实现还原英文单词原型\n",
    "st = PorterStemmer()\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join([word for word in x.split()]))\n",
    "#data['text'] = data['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "#分出训练集和测试集\n",
    "train=data[:5000]\n",
    "test=data[5000:]\n",
    "# 每个序列的最大长度，多了截断，少了补0\n",
    "max_sequence_length = 300\n",
    "\n",
    "#只保留频率最高的前20000个词\n",
    "num_words = 20000\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 100\n",
    "# 找出经常出现的单词，分词器\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(train.text)\n",
    "train_sequences = tokenizer.texts_to_sequences(train.text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.text)\n",
    "\n",
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# print(tokenizer.word_index)\n",
    "# total words in the corpus\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# get only the top frequent words on train\n",
    "\n",
    "train_x = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "# get only the top frequent words on test\n",
    "test_x = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "\n",
    "def lable_vectorize(labels):\n",
    "    label_vec = np.zeros([len(labels), 2])\n",
    "    for i, label in enumerate(labels):\n",
    "        if str(label) == 'ham':\n",
    "            label_vec[i][0] = 1\n",
    "        else:\n",
    "            label_vec[i][1] = 1\n",
    "    return label_vec\n",
    "\n",
    "train_y = lable_vectorize(train['label'])\n",
    "test_y = lable_vectorize(test['label'])\n",
    "'''\n",
    "num_words = 20000\n",
    "embedding_dim = 100\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    input_length=max_sequence_length))\n",
    "#model.add(Embedding(max_sequence_length,128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "plot_model(model, to_file='model3.png',show_shapes='True')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "            batch_size=64,\n",
    "            epochs=20,\n",
    "            validation_split=0.2,\n",
    "          callbacks=[PlotLossesKeras()])\n",
    "print(model.evaluate(test_x, test_y))\n",
    "# prediction on test data\n",
    "predicted=model.predict(test_x)\n",
    "print(predicted)\n",
    "\n",
    "#模型评估\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(test_y,predicted.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(test_y,predicted.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecebcdc-e97d-4b8d-b7ac-80184ae495d4",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271a07f-c8f6-4e10-8c7e-1c6702beaafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "os.environ[\"PATH\"] += os.pathsep +'C:\\Program Files\\Graphviz\\bin'\n",
    "#=============================================================================================\n",
    "#=============================================================================================\n",
    "# 读取数据\n",
    "data = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "# 去除无用数据，后3列是无用数据\n",
    "data = data[['v1', 'v2']]\n",
    "# 修改表头信息\n",
    "data = data.rename(columns={\"v1\":\"label\",\"v2\":\"text\"})\n",
    "# 去除标点符号及两个以上的空格\n",
    "data['text'] = data['text'].apply(lambda x:re.sub('[!@#$:).;,?&]', ' ', x.lower()))\n",
    "data['text'] = data['text'].apply(lambda x:re.sub(' ', ' ', x))\n",
    "# 单词转换为小写\n",
    "data['text'] = data['text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "# 去除停止词 ，如a、an、the、高频介词、连词、代词等\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# 分词处理，希望能够实现还原英文单词原型\n",
    "st = PorterStemmer()\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join([word for word in x.split()]))\n",
    "#data['text'] = data['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "#分出训练集和测试集\n",
    "train=data[:5000]\n",
    "test=data[5000:]\n",
    "# 每个序列的最大长度，多了截断，少了补0\n",
    "max_sequence_length = 300\n",
    "\n",
    "#只保留频率最高的前20000个词\n",
    "num_words = 20000\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 100\n",
    "# 找出经常出现的单词，分词器\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(train.text)\n",
    "train_sequences = tokenizer.texts_to_sequences(train.text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.text)\n",
    "\n",
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# get only the top frequent words on train\n",
    "\n",
    "train_x = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "# get only the top frequent words on test\n",
    "test_x = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "\n",
    "# 标签向量化\n",
    "# [0,1]: ham;[1,0]:spam\n",
    "def lable_vectorize(labels):\n",
    "    label_vec = np.zeros([len(labels), 2])\n",
    "    for i, label in enumerate(labels):\n",
    "        if str(label) == 'ham':\n",
    "            label_vec[i][0] = 1\n",
    "        else:\n",
    "            label_vec[i][1] = 1\n",
    "    return label_vec\n",
    "\n",
    "\n",
    "train_y = lable_vectorize(train['label'])\n",
    "test_y = lable_vectorize(test['label'])\n",
    "X_train = np.reshape(train_x , (train_x .shape[0], train_x .shape[1], 1))\n",
    "X_test = np.reshape(test_x, (test_x .shape[0], test_x .shape[1], 1))\n",
    "print(\"加载数据完成\")\n",
    "#=============================================================================================\n",
    "#=============================================================================================\n",
    "# 数据长度 一行有28个像素\n",
    "input_size = 28\n",
    "# 序列的长度\n",
    "time_steps = 28\n",
    "# 隐藏层block的个数\n",
    "cell_size = 64\n",
    "model = keras.Sequential()\n",
    "#直接添加SimpleRNN\n",
    "model.add(keras.layers.SimpleRNN(\n",
    "        units = cell_size, # 输出\n",
    "        input_shape=(max_sequence_length,1), # 输入\n",
    "   ))\n",
    "# 输出层\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "plot_model(model, to_file='SimpleRNN.png',show_shapes='True')\n",
    "model.summary()\n",
    "\n",
    "# 定义优化器\n",
    "adam = keras.optimizers.Adam(lr=1e-4)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# 训练模型\n",
    "model.fit(X_train, train_y, batch_size=64, epochs=4)\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(X_test,test_y)\n",
    "print('test loss', loss)\n",
    "print('test accuracy', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38kernel",
   "language": "python",
   "name": "py38-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
