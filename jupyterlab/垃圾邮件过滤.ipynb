{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13c477c-c482-4626-9423-516710383645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from email import parser, policy\n",
    "from html import unescape\n",
    "\n",
    "import nltk\n",
    "import pandas\n",
    "import urlextract\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics, preprocessing, naive_bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87cf93e0-b06d-4413-9cd5-fb76dd30cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "INDEX_PATH = os.path.join('trec07p', 'delay', 'index')  # 先使用较小的数据集进行训练\n",
    "DATA_PATH = os.path.join('trec07p', 'data')  # 数据文件夹路径\n",
    "labels = []\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cdf8616-3473-4ad7-9076-d7c1c8b3f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(index_path):\n",
    "    with open(index_path) as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.split(' ')\n",
    "            labels.append(line[0])\n",
    "            filenames.append(line[1].strip('\\n').split('/')[-1])\n",
    "            \n",
    "create_dataset(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19fad2d-ac67-4279-8de1-66edf6053323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, i've just updated from the gulus and I check on other mirrors.\n",
      "It seems there is a little typo in /debian/README file\n",
      "\n",
      "Example:\n",
      "http://gulus.usherbrooke.ca/debian/README\n",
      "ftp://ftp.fr.debian.org/debian/README\n",
      "\n",
      "\"Testing, or lenny.  Access this release through dists/testing.  The\n",
      "current tested development snapshot is named etch.  Packages which\n",
      "have been tested in unstable and passed automated tests propogate to\n",
      "this release.\"\n",
      "\n",
      "etch should be replace by lenny like in the README.html\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "Yan Morin\n",
      "Consultant en logiciel libre\n",
      "yan.morin@savoirfairelinux.com\n",
      "514-994-1556\n",
      "\n",
      "\n",
      "-- \n",
      "To UNSUBSCRIBE, email to debian-mirrors-REQUEST@lists.debian.org\n",
      "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n"
     ]
    }
   ],
   "source": [
    "def load_email(filename, file_path):\n",
    "    with open(os.path.join(file_path, filename), 'rb') as f:\n",
    "        return parser.BytesParser(policy=policy.default).parse(f)\n",
    "\n",
    "\n",
    "raw_emails = [load_email(name, DATA_PATH) for name in filenames]\n",
    "\n",
    "print(raw_emails[1].get_content().strip())  # 打印邮件文本内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfdc1ec9-d079-41dd-acfa-a5a42acb83ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text/plain', 29989), ('multipart(text/plain, text/html)', 17496), ('text/html', 11978), ('multipart(multipart(text/plain, text/html), image/gif)', 5066), ('multipart(text/html)', 3741), ('multipart(multipart(text/plain, text/html), image/jpeg)', 2510), ('multipart(text/html, image/gif)', 1021), ('multipart(text/plain)', 682), ('multipart(text/plain, application/pgp-signature)', 595), ('multipart(text/plain, text/plain)', 398), ('multipart(multipart(text/plain, text/html), image/png)', 240), ('multipart(multipart(text/plain, text/html))', 151), ('multipart(multipart(text/plain, text/html), text/plain)', 135), ('multipart(text/html, text/plain, image/png)', 122), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/jpeg, application/octet-stream)', 116), ('multipart(text/plain charset=us-ascii, text/html)', 108), ('multipart(text/plain, application/octet-stream)', 75), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, application/octet-stream)', 60), ('multipart(text/plain, text/x-patch)', 49), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/jpeg, image/jpeg)', 49), ('multipart(text/plain, text/x-diff)', 46), ('multipart(multipart(multipart(text/plain, text/html), image/gif), text/plain)', 45), ('multipart(text/plain, application/pdf)', 44), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(text/html))', 40), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(multipart(text/plain, text/html), image/gif)))', 38), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/jpeg, image/gif, application/octet-stream, image/gif, image/jpeg, image/gif)', 35), ('multipart(multipart(text/plain, application/pgp-signature), text/plain)', 34), ('multipart(multipart(text/plain, text/html), image/jpg)', 33), ('multipart(text/plain, text/plain, text/plain)', 30), ('multipart(text/plain, text/html, image/gif)', 29), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif)', 27), ('multipart(text/html, image/jpeg)', 23), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/gif, image/gif, image/gif)', 20), ('multipart(multipart(text/html))', 18), ('multipart(multipart(text/plain, text/plain), application/pgp-signature)', 17), ('multipart(multipart(text/plain, text/x-patch), application/pgp-signature)', 16), ('multipart(multipart(text/html), image/gif)', 16), ('multipart(text/plain, application/x-msdownload)', 15), ('multipart(multipart(text/plain, text/html), application/octet-stream)', 13), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/jpeg, image/gif)', 12), ('multipart(multipart(text/plain, text/html), image/gif, image/gif)', 12), ('multipart(multipart(multipart(text/plain, text/html), image/jpeg), text/plain)', 11), ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)', 10), ('multipart(multipart(text/plain))', 10), ('multipart(multipart(text/plain, application/x-pkcs7-signature), text/plain)', 9), ('multipart(text/plain, multipart(text/html))', 9), ('multipart(text/plain, application/x-pkcs7-signature)', 9), ('multipart(multipart(text/plain, text/x-diff), application/pgp-signature)', 9), ('multipart(text/plain, text/x-patch, application/pgp-signature)', 8), ('multipart(multipart(text/html, image/gif), application/octet-stream)', 8), ('multipart(text/plain, application/octet-stream, application/octet-stream)', 8), ('multipart(multipart(text/plain), text/plain)', 7), ('multipart(multipart(text/plain), application/octet-stream)', 7), ('multipart(text/plain, text/x-patch, text/plain)', 7), ('multipart(text/html, application/octet-stream)', 6), ('multipart(text/html, image/png)', 6), ('multipart(text/plain, text/html, text/plain)', 6), ('multipart(text/plain, text/x-patch, text/x-patch)', 5), ('multipart(text/plain, multipart(text/plain))', 5), ('multipart(text/html, text/html)', 5), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(text/plain))', 5), ('multipart(text/plain, text/plain, text/plain, text/plain)', 4), ('multipart(multipart(text/html), image/jpeg)', 4), ('multipart(text/plain, application/pdf, text/plain)', 4), ('multipart(text/html, image/jpeg, image/jpeg, image/jpeg)', 4), ('multipart(text/plain, multipart(text/html, image/gif))', 4), ('multipart(text/plain, multipart(text/plain), text/plain)', 3), ('multipart(multipart(text/plain, text/html), application/pgp-signature)', 3), ('multipart(text/plain, text/x-patch, application/octet-stream)', 3), ('multipart(multipart(text/plain, multipart(text/html)))', 3), ('multipart(multipart(text/plain, text/html), image/jpeg, image/gif)', 3), ('multipart(text/plain, text/x-diff, text/plain, text/plain)', 3), ('multipart(multipart(text/plain, text/html), image/gif, image/jpeg, image/gif)', 3), ('multipart(text/plain, text/x-log, text/plain)', 2), ('multipart(text/plain, image/png, text/plain)', 2), ('multipart(text/plain, multipart(text/plain), multipart(text/html))', 2), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/jpeg)', 2), ('multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/gif)', 2), ('multipart(text/plain, text/x-diff, text/plain)', 2), ('multipart(text/plain, text/plain, application/pgp-signature)', 2), ('multipart(multipart(text/plain, text/html), image/jpeg, image/gif, image/gif, image/jpeg, image/jpeg, image/jpeg, image/jpeg)', 2), ('multipart(multipart(multipart(text/plain, text/x-diff, text/x-diff, text/x-diff), application/pgp-signature), text/plain)', 2), ('multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream)', 2), ('multipart(multipart(text/html), text/plain)', 2), ('multipart(text/plain, text/x-log, text/x-log, text/plain)', 2), ('multipart/alternative', 2), ('multipart(text/plain, plain/text)', 1), ('multipart(multipart(text/plain, text/x-patch, text/x-patch), application/pgp-signature)', 1), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, text/html)))', 1), ('multipart(text/html, text/richtext)', 1), ('multipart(text/plain, text/x-diff, text/x-diff)', 1), ('multipart(multipart(text/plain, text/html, image/gif))', 1), ('multipart(multipart(text/plain, application/x-bzip), application/pgp-signature)', 1), ('multipart(multipart(multipart(text/plain)))', 1), ('multipart(text/plain, text/x-patch, text/x-patch, text/x-patch, text/x-patch)', 1), ('multipart(text/plain, text/plain, text/plain, text/plain, text/plain, text/plain, text/plain, text/plain, text/plain, text/plain)', 1), ('multipart(text/plain, application/octet-stream, text/plain)', 1), ('multipart(text/html, multipart(text/html))', 1), ('multipart(text/plain, text/html, application/octet-stream)', 1), ('multipart(text/plain, application/x-gzip, application/pgp-signature)', 1), ('multipart(text/plain, text/x-vcard, text/plain)', 1), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, application/octet-stream)', 1), ('multipart(multipart(text/plain, multipart(text/plain), multipart(text/html)), text/plain)', 1), ('multipart(text/plain, text/x-patch, text/troff)', 1), ('multipart(multipart(multipart(text/plain, text/x-diff, text/x-diff, text/x-diff, text/x-diff), application/pgp-signature), text/plain)', 1), ('multipart(text/plain, application/x-gzip)', 1), ('multipart(text/plain, image/tiff, text/plain)', 1), ('multipart(multipart(text/plain, text/html), application/octet-stream, application/octet-stream, application/octet-stream)', 1), ('multipart(text/plain, text/x-csrc, text/plain)', 1), ('multipart(text/plain, text/x-csrc, text/x-chdr, text/x-patch, application/pgp-signature)', 1), ('multipart(multipart(text/plain, text/html), image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg, image/jpeg)', 1), ('multipart(multipart(text/plain, text/x-csrc, application/x-perl, application/octet-stream), application/pgp-signature)', 1), ('multipart(multipart(text/plain, text/x-csrc, application/octet-stream, application/octet-stream), application/pgp-signature)', 1), ('multipart(multipart(text/plain, text/html), image/jpeg, text/plain)', 1), ('multipart(text/plain, image/jpeg, text/plain)', 1), ('multipart(multipart(text/plain, text/x-csrc), application/pgp-signature)', 1), ('multipart(text/plain, application/zip)', 1), ('multipart(text/plain, multipart(text/plain), multipart(text/plain))', 1), ('multipart(multipart(multipart(text/plain, text/plain)), text/plain)', 1), ('multipart(text/plain, application/pdf, text/plain, text/plain)', 1), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif, image/gif, image/gif, application/octet-stream, image/gif, image/gif, image/jpeg, image/jpeg)', 1), ('multipart(multipart(text/html, image/png, image/png), text/plain)', 1), ('multipart(text/plain, text/x-perl, text/plain)', 1), ('multipart(multipart(multipart(text/plain, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff, text/x-diff), application/pgp-signature), text/plain)', 1), ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)', 1), ('multipart(text/plain, multipart(multipart(text/plain, image/jpeg)))', 1), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(multipart(text/plain, text/html), image/jpeg)))', 1), ('multipart(multipart(text/plain, text/x-chdr), application/pgp-signature)', 1), ('multipart(multipart(text/plain), image/gif)', 1), ('multipart(text/html, application/octet-stream, application/octet-stream)', 1), ('multipart(text/plain, application/x-perl, text/plain)', 1), ('multipart/alternreturn-path: <accounting@aaawnca.com>', 1), ('multipart(text/plain, text/plain, text/plain, text/plain, text/plain, text/plain, text/plain)', 1), ('multipart(text/plain, multipart(application/applefile, text/plain), text/plain)', 1), ('multipart(multipart(text/plain, text/x-objcsrc, text/plain), application/pgp-signature)', 1), ('multipart(text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html, text/html)', 1), ('multipart(multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(multipart(text/plain, text/html), image/jpeg))), text/plain)', 1), ('multipart(text/plain, application/x-tar)', 1), ('multipart(multipart(text/plain, text/html), image/gif, image/gif, image/gif)', 1), ('multipart(text/plain, application/octet-stream, application/octet-stream, application/octet-stream, application/octet-stream)', 1), ('multipart(text/plain, text/plain, application/octet-stream)', 1), ('multipart(multipart(multipart(text/plain, text/plain), application/pgp-signature), text/plain)', 1), ('text/', 1), ('multipart(multipart(text/plain, text/html), application/octet-stream, text/plain)', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# 构造函数获取邮件的结构类型及其计数\n",
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return 'multipart({})'.format(', '.join([get_email_structure(sub_email) for sub_email in payload]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub(r'<[aA]\\s.*?>', 'HYPERLINK', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub(r'<img\\s.*?>', 'IMAGE', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)\n",
    "\n",
    "\n",
    "def email_to_text(email):\n",
    "    html = None\n",
    "    # walk()打印出一封具有多部分结构之信息的每个部分的MIME类型\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if ctype not in ('text/plain', 'text/html'):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except LookupError:\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == 'text/plain':\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)\n",
    "\n",
    "print(structures_counter(raw_emails).most_common())  # 显示邮件包含的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7736d2-7741-415a-9f4b-bdd8dcc7c301",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86180/nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\share\\\\nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86180\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ANACONDA\\envs\\py38\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ANACONDA\\envs\\py38\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86180/nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\share\\\\nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86180\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 分词\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stopwords_list \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 去英文停用词\u001b[39;00m\n\u001b[0;32m      3\u001b[0m token \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mSnowballStemmer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 提取词干\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m single \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m97\u001b[39m, \u001b[38;5;241m123\u001b[39m):\n",
      "File \u001b[1;32mC:\\ANACONDA\\envs\\py38\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\ANACONDA\\envs\\py38\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\ANACONDA\\envs\\py38\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ANACONDA\\envs\\py38\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\86180/nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\share\\\\nltk_data'\n    - 'C:\\\\ANACONDA\\\\envs\\\\py38\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\86180\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "stopwords_list = stopwords.words('english')  # 去英文停用词\n",
    "token = nltk.stem.SnowballStemmer('english')  # 提取词干\n",
    "for single in range(97, 123):\n",
    "    stopwords_list.append(chr(single))\n",
    "extractor = urlextract.URLExtract()\n",
    "\n",
    "def word_split(email):\n",
    "    text = email_to_text(email) or ' '\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "    urls = list(set(extractor.find_urls(text)))\n",
    "    urls.sort(key=lambda item: len(item), reverse=True)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, \"URL\")\n",
    "    text = re.sub(r'\\d+(?:\\.\\d*[eE]\\d+)?', 'NUMBER', text)\n",
    "    content = list(nltk.word_tokenize(text))\n",
    "    all_words = []\n",
    "    for word in content:\n",
    "        if word not in stopwords_list:\n",
    "            word = token.stem(word)\n",
    "            all_words.append(word)\n",
    "    return all_words\n",
    "\n",
    "all_emails = [word_split(data) for data in raw_emails]\n",
    "print(all_emails[0])  # 查看分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfc6548-cca5-472a-ac32-feff430db896",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_emails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 特征提取\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 创建一个dataframe，列名为text和label\u001b[39;00m\n\u001b[0;32m      3\u001b[0m trainDF \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 4\u001b[0m trainDF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mall_emails\u001b[49m\n\u001b[0;32m      5\u001b[0m trainDF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_emails' is not defined"
     ]
    }
   ],
   "source": [
    "# 特征提取\n",
    "# 创建一个dataframe，列名为text和label\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = all_emails\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064de1ef-8395-412c-935a-0cdcbf0f44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集分为测试集和训练集\n",
    "# sklearn.model_selection.train_test_split\n",
    "train_data, test_data, train_label, test_label = train_test_split(trainDF['text'],trainDF['label'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb452a63-253f-4fbb-9284-ae6bd1033af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label编码为目标变量,即从字符串转为一个数字\n",
    "# sklearn.preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_label = encoder.fit_transform(train_label)\n",
    "test_label = encoder.fit_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4730d1f-9b83-4750-a3ae-7887fbbccba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 计数特征向量\n",
    "# sklearn.feature_extraction.text.CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "count_vect.fit(trainDF['text'])\n",
    "xtrain_count = count_vect.transform(train_data)  # 训练集特征向量\n",
    "xtest_count = count_vect.transform(test_data)  # 测试集特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a341d-16de-425f-bef0-31604d37160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 TF-IDF特征向量\n",
    "# sklearn.feature_extraction.text.TfidfVectorizer\n",
    "# 4.2.1 词语级\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# 4.2.2 多词语级\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2, 3), max_features=5000)\n",
    "# 4.2.3 词性级\n",
    "tfidf_vect_char = TfidfVectorizer(analyzer='char', ngram_range=(2, 3), max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb3418-e597-4004-9fa8-90d8f3f958db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类任务\n",
    "# 创建分类器\n",
    "# sklearn.metrics\n",
    "def train_model(classifier, train_feature, test_feature):\n",
    "    classifier.fit(train_feature, train_label)\n",
    "    prediction = classifier.predict(test_feature)\n",
    "    acc = metrics.accuracy_score(prediction, test_label)\n",
    "    prec = metrics.precision_score(prediction, test_label)\n",
    "    rec = metrics.recall_score(prediction, test_label)\n",
    "    f1 = metrics.f1_score(prediction, test_label)\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbfab3a-94c4-4e88-a9dd-09ad4f5e9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 朴素贝叶斯多项式模型\n",
    "# 5.1.1 计数特征向量\n",
    "# Sklearn.naive_bayes\n",
    "accuracy, precision, recall, f1_socre = train_model(naive_bayes.MultinomialNB(), xtrain_count, xtest_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38kernel",
   "language": "python",
   "name": "py38-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
